{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":[],"metadata":{"id":"VsNZzVWU499D"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1yw22uSw1UQF"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import kagglehub\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n"]},{"cell_type":"markdown","source":["# **Loading the Dataset**"],"metadata":{"id":"ZXOONNwL42Vy"}},{"cell_type":"code","source":["df = pd.read_csv('/content/creditcard_dataset.csv')\n","df.info()"],"metadata":{"id":"AP5ug-Qo3ivv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df=df.dropna()"],"metadata":{"id":"5Hn5Qe7B8ty3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.isnull().sum()"],"metadata":{"id":"4iUOpO9380G2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fraud_counts = df['is_fraud'].value_counts()\n","sns.barplot(x=fraud_counts.index, y=fraud_counts.values)\n","plt.title('Distribution of Fraud')\n","plt.xlabel('Fraud')\n","plt.ylabel('Count')\n","for i, count in enumerate(fraud_counts.values):\n","    plt.text(i, count + 50, str(count), ha='center', va='bottom')\n","plt.show()"],"metadata":{"id":"FUZqHG6X9iSJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.duplicated().sum()"],"metadata":{"id":"4tk3oxVC-IBl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Data Preprocessing**"],"metadata":{"id":"NS3gl4Fg-S2Z"}},{"cell_type":"code","source":["fraud=df[df[\"is_fraud\"]==1]\n","not_fraud=df[df[\"is_fraud\"]==0]\n","print(fraud.shape[0])\n","print(not_fraud.shape[0])"],"metadata":{"id":"vHzOs06M-MOF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The Below code is balancing an imbalanced dataset by undersampling the majority class (non-fraudulent transactions) to match the number of samples in the minority class (fraudulent transactions). First, it randomly selects a subset of non-fraudulent samples to have the same number of rows as the fraudulent samples. This way, the not_fraud DataFrame now has an equal number of rows to fraud. Then, it combines this subset of non-fraudulent samples with the fraudulent samples to create a new balanced dataset, data. This approach, known as undersampling, helps ensure that the model doesnâ€™t become biased toward predicting the majority class (non-fraud), thereby improving its ability to detect fraud accurately."],"metadata":{"id":"x1lvQTCM_NJc"}},{"cell_type":"code","source":["not_fraud=not_fraud.sample(fraud.shape[0])\n","data=pd.concat([fraud,not_fraud])"],"metadata":{"id":"lg9gIrrX-moB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Graph Reprasentation of New Distribution"],"metadata":{"id":"jyjsPTSoADN3"}},{"cell_type":"code","source":["fraud_counts=data['is_fraud'].value_counts()\n","sns.barplot(x=fraud_counts.index,y=fraud_counts.values)\n","plt.title('New Distribution of Fraud')\n","plt.xlabel('Fraud')\n","plt.ylabel('Count')\n","for i, count in enumerate(fraud_counts.values):\n","    plt.text(i, count + 50, str(count), ha='center', va='bottom')\n","plt.show()"],"metadata":{"id":"9PmWfvaq_n9E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Feature Engineering**"],"metadata":{"id":"lliosjkgA9kb"}},{"cell_type":"code","source":["unused_cols=['Unnamed: 0','first','last','unix_time','street','gender','job','dob','city','state','trans_num','merchant']\n","data.drop(columns=unused_cols,inplace=True)"],"metadata":{"id":"XnwWa9TwABU7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.info()"],"metadata":{"id":"gT7MqZkCBUm3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data['trans_date_trans_time']=pd.to_datetime(data['trans_date_trans_time'])\n","data['trans_day']=data['trans_date_trans_time'].dt.day\n","data['trans_month']=data['trans_date_trans_time'].dt.month\n","data['trans_year']=data['trans_date_trans_time'].dt.year\n","data['trans_hour']=data['trans_date_trans_time'].dt.hour\n","data['trans_minute']=data['trans_date_trans_time'].dt.minute\n","data.drop(columns=['trans_date_trans_time'],inplace=True)"],"metadata":{"id":"FT4gE65GBcGZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import LabelEncoder\n","encoder=LabelEncoder()\n","data['category']=encoder.fit_transform(data['category'])\n","data['cc_num']=encoder.fit_transform(data['cc_num'])\n","data.head()"],"metadata":{"id":"cp4HFvPWBgmq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scaler=StandardScaler()\n","data['amt']=scaler.fit_transform(data[['amt']])\n","data['zip']=scaler.fit_transform(data[['zip']])\n","data['city_pop']=scaler.fit_transform(data[['city_pop']])\n","data['cc_num']=encoder.fit_transform(data['cc_num'])"],"metadata":{"id":"wMG6n02qB_PG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X=data.drop('is_fraud',axis=1)\n","y=data['is_fraud']"],"metadata":{"id":"iAklWXmlCITx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Splittind Data Set**"],"metadata":{"id":"5pw1zrVP5gYp"}},{"cell_type":"markdown","source":["This code splits the dataset into training (30%) and testing (70%) sets. Setting `random_state=0` ensures the split is the same each time, making results reproducible."],"metadata":{"id":"i0NSTwz2Cra3"}},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7,random_state=0)"],"metadata":{"id":"9_eynFAzCNOq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Building the Model**"],"metadata":{"id":"clwJYnHfDZ6R"}},{"cell_type":"markdown","source":["**DecisionTree**"],"metadata":{"id":"K5Cvi9dYg33t"}},{"cell_type":"code","source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score , precision_score ,recall_score ,f1_score\n","# Train Decision Tree model\n","dt_model = DecisionTreeClassifier(random_state=42)\n","dt_model.fit(X_train, y_train)\n","y_pred_dt = dt_model.predict(X_test)\n","\n","# Print metrics for Decision Tree\n","print(\"Decision Tree\")\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred_dt))\n","print(\"Precision:\", precision_score(y_test, y_pred_dt))\n","print(\"Recall:\", recall_score(y_test, y_pred_dt))\n","print(\"F1 Score:\", f1_score(y_test, y_pred_dt))\n","print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_dt))\n","print(\"\\n\")\n"],"metadata":{"id":"SQyJXILDCwOR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Random Forest**"],"metadata":{"id":"OvwuWQGNg8ld"}},{"cell_type":"code","source":["# Train Random Forest model\n","rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf_model.fit(X_train, y_train)\n","y_pred_rf = rf_model.predict(X_test)\n","\n","# Print metrics for Random Forest\n","print(\"Random Forest\")\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n","print(\"Precision:\", precision_score(y_test, y_pred_rf))\n","print(\"Recall:\", recall_score(y_test, y_pred_rf))\n","print(\"F1 Score:\", f1_score(y_test, y_pred_rf))\n","print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n","print(\"\\n\")\n"],"metadata":{"id":"45X4yhFNghoF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Gradient Boosting**"],"metadata":{"id":"iSgOURHRhA_s"}},{"cell_type":"code","source":["# Train Gradient Boosting model\n","gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n","gb_model.fit(X_train, y_train)\n","y_pred_gb = gb_model.predict(X_test)\n","\n","# Print metrics for Gradient Boosting\n","print(\"Gradient Boosting\")\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred_gb))\n","print(\"Precision:\", precision_score(y_test, y_pred_gb))\n","print(\"Recall:\", recall_score(y_test, y_pred_gb))\n","print(\"F1 Score:\", f1_score(y_test, y_pred_gb))\n","print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_gb))\n","print(\"\\n\")\n"],"metadata":{"id":"GQ-fW9bzgmOU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Logistic Regression**"],"metadata":{"id":"_kudYKhsI4Ny"}},{"cell_type":"code","source":["# Train Logistic Regression model\n","lr_model = LogisticRegression()\n","lr_model.fit(X_train, y_train)\n","y_pred_lr = lr_model.predict(X_test)\n","\n","# Print metrics for Logistic Regression\n","print(\"Logistic Regression\")\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr))\n","print(\"Precision:\", precision_score(y_test, y_pred_lr))\n","print(\"Recall:\", recall_score(y_test, y_pred_lr))\n","print(\"F1 Score:\", f1_score(y_test, y_pred_lr))\n","print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_lr))\n","print(\"\\n\")\n"],"metadata":{"id":"rPH-OBRHguWQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Visualizing the Results & Comparing the Models**"],"metadata":{"id":"KSFy4lBt5_A-"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Function to plot confusion matrix\n","def plot_confusion_matrix(y_test, y_pred, title):\n","    plt.figure(figsize=(6, 6))\n","    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n","    plt.title(f'Confusion Matrix: {title}')\n","    plt.xlabel('Predicted')\n","    plt.ylabel('Actual')\n","    plt.show()\n","\n","# Plotting confusion matrices\n","plot_confusion_matrix(y_test, y_pred_lr, 'Logistic Regression')\n","plot_confusion_matrix(y_test, y_pred_dt, 'Decision Tree')\n","plot_confusion_matrix(y_test, y_pred_rf, 'Random Forest')\n","plot_confusion_matrix(y_test, y_pred_gb, 'Gradient Boosting')\n"],"metadata":{"id":"BotG4mWoiZN0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to print and return metrics\n","def get_metrics(y_test, y_pred):\n","    accuracy = accuracy_score(y_test, y_pred)\n","    precision = precision_score(y_test, y_pred)\n","    recall = recall_score(y_test, y_pred)\n","    f1 = f1_score(y_test, y_pred)\n","    cm = confusion_matrix(y_test, y_pred)\n","    return accuracy, precision, recall, f1, cm\n","\n","# Dictionary to store all metrics\n","metrics_dict = {\n","    'Logistic Regression': get_metrics(y_test, y_pred_lr),\n","    'Decision Tree': get_metrics(y_test, y_pred_dt),\n","    'Random Forest': get_metrics(y_test, y_pred_rf),\n","    'Gradient Boosting': get_metrics(y_test, y_pred_gb)\n","}\n","\n","# Display metrics\n","metrics_df = pd.DataFrame(metrics_dict, index=['Accuracy', 'Precision', 'Recall', 'F1 Score', 'Confusion Matrix']).transpose()\n","metrics_df = metrics_df.drop(columns=['Confusion Matrix'])  # Drop confusion matrix for graphical representation\n","print(metrics_df)\n","\n","\n","# Plot the metrics\n","metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n","fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n","\n","for ax, metric in zip(axes.flatten(), metrics):\n","    metrics_df[metric].plot(kind='bar', ax=ax)\n","    for i in range(len(metrics_df)):\n","        ax.text(i, metrics_df[metric][i] + 0.01, round(metrics_df[metric][i], 4), ha='center')\n","    ax.set_title(f'{metric} Scores by Model')\n","    ax.set_ylabel(metric)\n","    ax.set_ylim(0, 1)\n","    ax.set_xticks(range(len(metrics_df)))\n","    ax.set_xticklabels(metrics_df.index, rotation=45)\n","\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"6s9zYvIrhHmV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Based on your results, Gradient Boosting has proven to be the best model for your credit card fraud detection project. It achieved the highest accuracy at 0.946720, and its precision of 0.965330 ensures that legitimate transactions are rarely flagged as fraudulent. With a recall of 0.928712, it effectively identifies most fraudulent transactions, and the balanced F1 score of 0.946667 further confirms its overall robustness and reliability. These metrics indicate that Gradient Boosting is highly effective in detecting fraudulent transactions while minimizing false positives and negatives. Its ability to handle complex data patterns and reduce overfitting makes it the optimal choice for your project."],"metadata":{"id":"-Ch5rgFB7wHw"}},{"cell_type":"code","source":["\n","# Initial evaluation\n","def evaluate_model(y_test, y_pred, model_name):\n","    print(f\"Metrics for {model_name}\")\n","    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n","    print(\"Precision:\", precision_score(y_test, y_pred))\n","    print(\"Recall:\", recall_score(y_test, y_pred))\n","    print(\"F1 Score:\", f1_score(y_test, y_pred))\n","    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n","    print(\"\\n\")\n","\n","evaluate_model(y_test, y_pred_gb, \"Default Gradient Boosting\")\n"],"metadata":{"id":"iEqKgZSFjH1u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Features That Helped in Model Prediction**"],"metadata":{"id":"KFLN3qXXoPfY"}},{"cell_type":"markdown","source":["**Interpretation**\n","\n","**High Importance Features:** These are the features that have the highest bars in the plot. They are the most influential in the model's decision-making process.\n","\n","**Low Importance Features:** These features have the smallest bars and contributed the least to the model's predictions. Sometimes, you may decide to remove these features to simplify the model without losing much predictive power."],"metadata":{"id":"DMxQ-_wFoeXn"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Get feature importances from the Gradient Boosting model\n","feature_importances = gb_model.feature_importances_\n","features = X.columns\n","indices = np.argsort(feature_importances)\n","\n","# Plot the feature importances\n","plt.figure(figsize=(10, 8))\n","plt.title('Feature Importances: Gradient Boosting')\n","plt.barh(range(len(indices)), feature_importances[indices], align='center')\n","plt.yticks(range(len(indices)), [features[i] for i in indices])\n","plt.xlabel('Relative Importance')\n","plt.show()\n"],"metadata":{"id":"uS1ZD03InkZL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Fine-Tuning and Improvements**"],"metadata":{"id":"2MdXb2Auzk1_"}},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","\n","# Define the parameter grid\n","param_grid = {\n","    'n_estimators': [100, 200, 300],\n","    'learning_rate': [0.01, 0.05, 0.1],\n","    'max_depth': [3, 4, 5],\n","    'min_samples_split': [2, 5, 10],\n","    'min_samples_leaf': [1, 2, 4],\n","    'subsample': [0.8, 0.9, 1.0],\n","    'max_features': ['auto', 'sqrt', 'log2']\n","}\n","\n","# Initialize the Gradient Boosting model\n","gb_model = GradientBoostingClassifier(random_state=42)\n","\n","# Initialize GridSearchCV\n","grid_search = GridSearchCV(estimator=gb_model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n","\n","# Fit the model\n","grid_search.fit(X_train, y_train)\n","\n","# Print the best parameters and the best score\n","print(\"Best Parameters:\", grid_search.best_params_)\n","print(\"Best Score:\", grid_search.best_score_)\n"],"metadata":{"id":"nfHmdFzKpXUS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import GridSearchCV\n","\n","# Define the parameter grid\n","param_grid = {\n","    'n_estimators': [100, 200, 300, 400, 500],\n","    'max_depth': [10, 20, 30, 40, 50, None],\n","    'min_samples_split': [2, 5, 10],\n","    'min_samples_leaf': [1, 2, 4],\n","    'criterion': ['gini', 'entropy']\n","}\n","# Initialize the Random Forest model\n","rf_model = RandomForestClassifier(random_state=42)\n","\n","# Initialize GridSearchCV\n","grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n","\n","# Fit the model\n","grid_search.fit(X_train, y_train)\n","\n","# Print the best parameters and the best score\n","print(\"Best Parameters:\", grid_search.best_params_)\n","print(\"Best Score:\", grid_search.best_score_)\n","# Get the best model from grid search\n","best_rf_model = grid_search.best_estimator_\n","\n","# Predict using the optimized model\n","y_pred_best_rf = best_rf_model.predict(X_test)\n","\n","# Print metrics for the optimized Random Forest\n","def print_metrics(y_test, y_pred, model_name):\n","    print(f\"Metrics for {model_name}\")\n","    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n","    print(\"Precision:\", precision_score(y_test, y_pred))\n","    print(\"Recall:\", recall_score(y_test, y_pred))\n","    print(\"F1 Score:\", f1_score(y_test, y_pred))\n","    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n","    print(\"\\n\")\n","\n","print_metrics(y_test, y_pred_best_rf, \"Optimized Random Forest\")\n"],"metadata":{"id":"9TFlZtCp0G9L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Conclusion**\n","\n","After thorough evaluation and comparison of the models, Gradient Boosting emerges as the best model for your credit card fraud detection project. Initially, Gradient Boosting showcased superior performance with high accuracy (0.944389), precision (0.962636), recall (0.92675), and F1 score (0.944352). Despite further hyperparameter optimization using Grid Search, this model maintained its strong performance, indicating robustness and effectiveness in handling complex data patterns and non-linear relationships crucial for fraud detection.\n","\n","Although Random Forest also demonstrated excellent performance, especially after optimization, Gradient Boosting consistently outperformed it across key metrics, making it the optimal choice for minimizing both false positives and false negatives in fraud detection. Therefore, deploying Gradient Boosting will provide you with the most reliable and accurate results for detecting fraudulent transactions in your dataset."],"metadata":{"id":"nfAcKyew-ZH1"}}]}